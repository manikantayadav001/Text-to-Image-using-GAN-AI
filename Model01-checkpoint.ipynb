{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the COCO2017 dataset\n",
    "captions_path = './annotations/captions_train2017.json'\n",
    "captions = json.load(open(captions_path))['annotations']\n",
    "texts = [caption['caption'] for caption in captions]\n",
    "\n",
    "# Tokenize the captions into words\n",
    "tokenized_texts = [nltk.word_tokenize(text) for text in texts]\n",
    "\n",
    "# Load a pre-trained Word2Vec model\n",
    "w2v_path = './w2vmodel.bin'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "\n",
    "# Convert each word to a Word2Vec embedding\n",
    "embedded_texts = []\n",
    "for text in tokenized_texts:\n",
    "    embedded_text = []\n",
    "    for word in text:\n",
    "        try:\n",
    "            embedding = w2v_model[word]\n",
    "            embedded_text.append(embedding)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    embedded_texts.append(embedded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the target image size\n",
    "target_size = (256, 256)\n",
    "\n",
    "# Initialize an empty list to store the processed embedded texts\n",
    "processed_embedded_texts = []\n",
    "\n",
    "# Process each embedded text\n",
    "for embedded_text in embedded_texts:\n",
    "    # Convert the embedded text to a PIL Image object\n",
    "    embedded_image = Image.fromarray(np.array(embedded_text))\n",
    "\n",
    "    # Resize the image to the target size\n",
    "    resized_image = embedded_image.resize(target_size)\n",
    "\n",
    "    # Convert the image to a NumPy array of pixel values\n",
    "    pixel_values = np.array(resized_image)\n",
    "\n",
    "    # Normalize the pixel values to be between 0 and 1\n",
    "    normalized_pixel_values = pixel_values / 255.0\n",
    "\n",
    "    # Add the processed embedded text to the list\n",
    "    processed_embedded_texts.append(normalized_pixel_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the preprocessed image feature vectors\n",
    "feature_vectors = np.load('image_features.npy')\n",
    "\n",
    "# Define the input shape for the CNN\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "# Define the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "# Train the model on the preprocessed text data\n",
    "history = model.fit(processed_embedded_texts, feature_vectors, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Reshape, Conv2D, Conv2DTranspose, Flatten, LeakyReLU, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define the dimensionality of the noise vector\n",
    "latent_dim = 100\n",
    "\n",
    "# Define the shape of the target image\n",
    "image_shape = (256, 256, 3)\n",
    "\n",
    "# set the directory where the images are stored\n",
    "image_dir = \"./img\"\n",
    "\n",
    "# create a list of image file paths in the directory\n",
    "image_paths = glob.glob(image_dir + \"/*.jpg\")\n",
    "\n",
    "\n",
    "# Define the generator model\n",
    "generator = Sequential()\n",
    "generator.add(Dense(128 * 64 * 64, input_dim=latent_dim))\n",
    "generator.add(Reshape((64, 64, 128)))\n",
    "generator.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "\n",
    "# Define the discriminator model\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(64, (3,3), padding='same', input_shape=image_shape))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Conv2D(64, (3,3), strides=(2,2), padding='same'))\n",
    "discriminator.add(BatchNormalization())\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "discriminator.add(BatchNormalization())\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
    "discriminator.add(BatchNormalization())\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Define the GAN by combining the generator and discriminator\n",
    "gan = Sequential()\n",
    "gan.add(generator)\n",
    "gan.add(discriminator)\n",
    "\n",
    "# Compile the discriminator model\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "\n",
    "# Freeze the weights of the discriminator during GAN training\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Compile the GAN model\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "\n",
    "# Define a function to generate a batch of noise vectors\n",
    "def generate_noise(batch_size, latent_dim):\n",
    "    return np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "# Define a function to generate a batch of real images\n",
    "def generate_real_images(batch_size, processed_embedded_texts, image_paths):\n",
    "    indices = np.random.randint(0, len(processed_embedded_texts), batch_size)\n",
    "    real_images = []\n",
    "    for i in indices:\n",
    "        image = Image.open(image_paths[i])\n",
    "        resized_image = image.resize((256, 256))\n",
    "        pixel_values = np.array(resized_image) / 255.0\n",
    "        real_images.append(pixel_values)\n",
    "    return np.array(real_images)\n",
    "\n",
    "#Define the number of epochs for training\n",
    "num_epochs = 100\n",
    "\n",
    "#Define the batch size for training\n",
    "batch_size = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the GAN\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "# Generate a batch of real images\n",
    "    real_images = generate_real_images(batch_size, processed_embedded_texts, image_paths)\n",
    "# Generate a batch of noise vectors\n",
    "noise = generate_noise(batch_size, latent_dim)\n",
    "\n",
    "# Use the generator to produce fake images from the noise\n",
    "fake_images = generator.predict(noise)\n",
    "\n",
    "# Concatenate the real and fake images into a single array\n",
    "X = np.concatenate([real_images, fake_images])\n",
    "\n",
    "# Create an array of labels indicating whether each image is real or fake\n",
    "y = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "\n",
    "# Train the discriminator on the combined real and fake images\n",
    "discriminator_loss, discriminator_accuracy = discriminator.train_on_batch(X, y)\n",
    "\n",
    "# Generate a new batch of noise vectors\n",
    "noise = generate_noise(batch_size, latent_dim)\n",
    "\n",
    "# Create an array of labels indicating that the generated images are real (but they are fake)\n",
    "misleading_targets = np.ones((batch_size, 1))\n",
    "\n",
    "# Train the GAN on the noise with the \"real\" labels\n",
    "gan_loss = gan.train_on_batch(noise, misleading_targets)\n",
    "\n",
    "# Print the loss and accuracy for the discriminator and the GAN at the end of each epoch\n",
    "print(f\"Epoch {epoch}/{num_epochs} -- Discriminator loss: {discriminator_loss:.4f}, Discriminator accuracy: {discriminator_accuracy:.4f}, GAN loss: {gan_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
